{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5580c7-6285-4005-a77b-4b3fcff8d2c9",
   "metadata": {},
   "source": [
    "## Working with data loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f407ae-3468-45e1-895d-8599e8363c68",
   "metadata": {},
   "source": [
    "*Olm* is designed to work seamlessly with data from data loggers. Most functions that conduct geochemical calculations can be run with arrays or *Pandas* `Series` objects as inputs. There are also a variety of convenience functions for reading datalogger csv files and postprocessing those data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62625efe-c5a0-4208-931e-b66ab8e14ab7",
   "metadata": {},
   "source": [
    "### The `olm.loggers` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36398d0c-f843-4a6b-9c74-4b20c2536ae1",
   "metadata": {},
   "source": [
    "`olm.loggers` provides a set of modules (called toolkits) for importing data from common data logger formats. All of these toolkits use the *Pandas* `read_csv()` function to read data into a Pandas `DataFrame`. While it is pretty easy to use customize `read_csv()` yourself to read in the desired datalogger file, these convenience functions make it easier for common formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf2535-9527-4f5f-8170-ff0175efbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If olm isn't already installed (or if you're running in Colab), then run this cell of code.\n",
    "!pip install olm-karst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946a280-dba2-498c-8555-fe355d99d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will run in pylab mode, to import plotting functions.\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca886d-564d-4ebd-b575-fddea9b8b658",
   "metadata": {},
   "source": [
    "#### Reading in logger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16576c-e96d-4391-81dc-ff71cdac4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use an example of reading data from an Onset HOBO data logger\n",
    "from olm.loggers.HoboToolkit import read_hobo_csv\n",
    "\n",
    "#Here we import temperature data from three HOBO loggers\n",
    "T1_jan = read_hobo_csv('data/2015-01-21/BS-T1.csv')\n",
    "T3_jan = read_hobo_csv('data/2015-01-21/BS-T3.csv')\n",
    "T5_jan = read_hobo_csv('data/2015-01-21/BS-T5.csv')\n",
    "\n",
    "#Plot the data as an example to compare the three timeseries\n",
    "T1_jan.Temp.plot()\n",
    "T3_jan.Temp.plot()\n",
    "T5_jan.Temp.plot()\n",
    "legend(['T1','T3','T5'])\n",
    "ylabel('Temperature ($^\\circ$C)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28204c24-882c-40c4-9679-ab5cab94529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we read in a couple more months (if we were clever, we'd use a for loop instead)\n",
    "T1_feb = read_hobo_csv('data/2015-02-19/BS-T1.csv')\n",
    "T3_feb = read_hobo_csv('data/2015-02-19/BS-T3.csv')\n",
    "T5_feb = read_hobo_csv('data/2015-02-19/BS-T5.csv')\n",
    "\n",
    "T1_mar = read_hobo_csv('data/2015-03-16/BS-T1.csv')\n",
    "T3_mar = read_hobo_csv('data/2015-03-16/BS-T3.csv')\n",
    "T5_mar = read_hobo_csv('data/2015-03-16/BS-T5.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50096ba0-4770-430a-b252-363cea304cf5",
   "metadata": {},
   "source": [
    "### The `olm.loggers.loggerScripts` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00ba48-7092-41b2-a6e1-c1c941e96451",
   "metadata": {},
   "source": [
    "#### Concatenating logger data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3858eb-1158-401e-95fd-dd46ee76fa36",
   "metadata": {},
   "source": [
    "Currently, we have separate *Pandas* DataFrames for each CSV file. We can use [`pandas.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to put together the total record for each logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbdf3c-3afd-4e8b-8f89-2e17dff861d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import concat\n",
    "\n",
    "#To concatenate the DataFrames for each logger, we just provide a list of the individual DataFrames\n",
    "#There are also many options described in the Pandas docs.\n",
    "T1 = concat([T1_jan, T1_feb, T1_mar])\n",
    "T3 = concat([T3_jan, T3_feb, T3_mar])\n",
    "T5 = concat([T5_jan, T5_feb, T5_mar])\n",
    "\n",
    "T1.Temp.plot()\n",
    "T3.Temp.plot()\n",
    "T5.Temp.plot()\n",
    "legend(['T1','T3','T5'])\n",
    "ylabel('Temperature ($^\\circ$C)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f7120-89f2-4d08-9c35-bf9831b174fd",
   "metadata": {},
   "source": [
    "#### Joining loggers into a single `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119bbf7-6087-4893-9637-eee3a26c7a64",
   "metadata": {},
   "source": [
    "Depending on how you have set them up, loggers will not always have the same timestamps. See the example below for T1 and T3, whose timestamps do not align. This causes problems when merging into a single `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2190e5e-17db-4c8e-bef4-ae0d67bdc07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data from T1')\n",
    "print(T1_jan.head())\n",
    "print('Data from T3')\n",
    "print(T3_jan.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4b1a1-16af-4bd3-8162-388c3f769ac2",
   "metadata": {},
   "source": [
    "#### Join and resample loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5aac2b-2f34-4dc7-b16d-a97d6a67f45a",
   "metadata": {},
   "source": [
    "olm.loggers.loggerScripts contains a [`joinAndResampleLoggers()`](https://olm.readthedocs.io/en/master/olm.loggers.html#olm.loggers.loggerScripts.joinAndResampleLoggers) function that enables simulataneous joining of loggers and resampling onto a common timestamp index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4fff8-6bda-4df1-8e9b-4d835533a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from olm.loggers.loggerScripts import joinAndResampleLoggers\n",
    "\n",
    "#Provide a list of the loggers to be joined as well as the interval\n",
    "# See list of frequency strings (e.g. min) that can be used in intervals here:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
    "# Since the three loggers contain a duplicate column name (Temp), we have to provide suffixes\n",
    "# for each one to differentiate the column names.\n",
    "df = joinAndResampleLoggers([T1,T3,T5], '5min', suffixes=['T1', 'T3', 'T5'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a7cc8-6cb7-42a6-8f52-e638b25e95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "ylabel('Temperature ($^\\circ$C)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5ecb4-c60b-44e1-a6c0-1b8f751adb8c",
   "metadata": {},
   "source": [
    "#### Applying linear corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3efc8f-33ba-4b69-b9c7-57df12f762a2",
   "metadata": {},
   "source": [
    "Sometimes data logger values drift due to bio-fouling or other factors (particularly conductivity loggers). This drift is often removed using a linear correction based on spot measurements at the site. [`olm.loggers.loggerScripts.linear_correction()`](https://olm.readthedocs.io/en/master/olm.loggers.html#olm.loggers.loggerScripts.linear_correction) provides a function to make such corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448b7f3-b79a-4a3a-a704-443ee9cb1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in some raw data logger data from a Campbell .dat file\n",
    "from olm.loggers.CampbellToolkit import read_dat\n",
    "Langle = read_dat('data/CR800_Langle_Water.dat')\n",
    "Langle_cond_datalogger = 1000*Langle.Cond_Avg #Convert to muS/cm from mS/cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2afa8c-e48d-4a0a-9850-e0a742b497a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the spot measurements from a csv file\n",
    "from pandas import read_csv\n",
    "spot_meas = read_csv('data/Langle_data_fixed.csv', parse_dates=[[0,1]], index_col=0, na_values='NaN')\n",
    "cond_spot = spot_meas['cond']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdce5b-df1f-4598-8cc9-611a496eb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from olm.loggers.loggerScripts import linear_correction\n",
    "#To correct, simply provide the timeseries DataFrame and spot measurement DataFrame\n",
    "#Both must have datetime indicies.\n",
    "cond_corr = linear_correction(Langle_cond_datalogger, cond_spot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4319242-fe12-4b18-9426-b1521fadc19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title('Comparing the raw and corrected series')\n",
    "Langle_cond_datalogger.plot()\n",
    "cond_corr.plot()\n",
    "cond_spot.plot(style='ko')\n",
    "legend(['Raw SpC', 'Corrected SpC', 'Spot measurements'], loc='lower right')\n",
    "ylabel('SpC ($\\mu S/cm$)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984582ed-b1a6-4af9-a9fc-7ffb8ff25785",
   "metadata": {},
   "source": [
    "#### Shifting a logger's timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd378f0a-ae8d-4ce6-b67b-5dbb70284d27",
   "metadata": {},
   "source": [
    "Sometimes the datalogger clock gets reset to the wrong value (wrong timezone, or some bizarre time in the distant future or past). As long as the offsets are correct between timestamps, it is easy to correct this shift. While this can be corrected easily with a few lines of code using Pandas functions, this happens frequently enough that I didn't want to have to reinvent the code each time. [`olm.loggers.loggerScripts.shiftLogger()` ](https://olm.readthedocs.io/en/master/olm.loggers.html#olm.loggers.loggerScripts.shiftLogger) will do this shift for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f2cd7-6baf-4c79-9937-5d19ba8e0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS4_cond = read_hobo_csv('data/2016-10-20/BS4-Cond.csv')\n",
    "BS4_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb210dd-2899-443d-9169-410fdd463056",
   "metadata": {},
   "source": [
    "This timestamps are in the distant future and can't be right. Actually the RTC chip in the HOBO Shuttle had gotten damaged by water, causing it to reset the loggers to this strange time. No worries, I know from my field notes that I downloaded this logger and restarted it last at 23:11 on 10/5/2016 UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c8ea2-0d49-4865-acb8-f982ed5ddd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from olm.loggers.loggerScripts import shiftLogger\n",
    "#Just provide the DataFrame to shift and the starting timestamp desired.\n",
    "#One can also shift to end at a specific time using align_at_start=False.\n",
    "BS4_cond_corrected = shiftLogger(BS4_cond, '10/05/2016 23:11',align_at_start=True)\n",
    "\n",
    "BS4_cond_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa76dac-70e7-42e7-9027-7ea6e64d727a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
